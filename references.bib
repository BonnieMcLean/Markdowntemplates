@article{dingemanse_what_2016,
	title = {What sound symbolism can and cannot do: {Testing} the iconicity of ideophones from five languages},
	volume = {92},
	shorttitle = {What sound symbolism can and cannot do},
	number = {2},
	journal = {Language},
	author = {Dingemanse, Mark and Schuerman, Will and Reinisch, Eva and Tufvesson, Sylvia and Mitterer, Holger},
	year = {2016},
	note = {Publisher: Linguistic Society of America},
	pages = {e117--e133},
	file = {Snapshot:C\:\\Users\\bonmc643\\Zotero\\storage\\MJACSRHS\\summary.html:text/html;Full Text:C\:\\Users\\bonmc643\\Zotero\\storage\\AWKM4RI6\\Dingemanse et al. - 2016 - What sound symbolism can and cannot do Testing th.pdf:application/pdf}
}

@article{peer_beyond_2017,
	title = {Beyond the {Turk}: {Alternative} platforms for crowdsourcing behavioral research},
	volume = {70},
	issn = {0022-1031},
	shorttitle = {Beyond the {Turk}},
	url = {http://www.sciencedirect.com/science/article/pii/S0022103116303201},
	doi = {10.1016/j.jesp.2017.01.006},
	abstract = {The success of Amazon Mechanical Turk (MTurk) as an online research platform has come at a price: MTurk has suffered from slowing rates of population replenishment, and growing participant non-naivety. Recently, a number of alternative platforms have emerged, offering capabilities similar to MTurk but providing access to new and more naïve populations. After surveying several options, we empirically examined two such platforms, CrowdFlower (CF) and Prolific Academic (ProA). In two studies, we found that participants on both platforms were more naïve and less dishonest compared to MTurk participants. Across the three platforms, CF provided the best response rate, but CF participants failed more attention-check questions and did not reproduce known effects replicated on ProA and MTurk. Moreover, ProA participants produced data quality that was higher than CF's and comparable to MTurk's. ProA and CF participants were also much more diverse than participants from MTurk.},
	language = {en},
	urldate = {2020-07-03},
	journal = {Journal of Experimental Social Psychology},
	author = {Peer, Eyal and Brandimarte, Laura and Samat, Sonam and Acquisti, Alessandro},
	month = may,
	year = {2017},
	keywords = {Amazon Mechanical Turk, CrowdFlower, Crowdsourcing, Data quality, Online research, Prolific Academic},
	pages = {153--163},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bonmc643\\Zotero\\storage\\A4DHQ3IR\\Peer et al. - 2017 - Beyond the Turk Alternative platforms for crowdso.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\bonmc643\\Zotero\\storage\\V2XK5LX8\\S0022103116303201.html:text/html}
}

@article{woods_headphone_2017,
	title = {Headphone screening to facilitate web-based auditory experiments},
	volume = {79},
	issn = {1943-3921},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5693749/},
	doi = {10.3758/s13414-017-1361-2},
	abstract = {Psychophysical experiments conducted remotely over the internet permit data collection from large numbers of participants, but sacrifice control over sound presentation, and therefore are not widely employed in hearing research. To help standardize online sound presentation, we introduce a brief psychophysical test for determining if online experiment participants are wearing headphones. Listeners judge which of three pure tones is quietest, with one of the tones presented 180° out of phase across the stereo channels. This task is intended to be easy over headphones but difficult over loudspeakers due to phase-cancellation. We validated the test in the lab by testing listeners known to be wearing headphones or listening over loudspeakers. The screening test was effective and efficient, discriminating between the two modes of listening with a small number of trials. When run online, a bimodal distribution of scores was obtained, suggesting that some participants performed the task over loudspeakers despite instructions to use headphones. The ability to detect and screen out these participants mitigates concerns over sound quality for online experiments, a first step toward opening auditory perceptual research to the possibilities afforded by crowdsourcing.},
	number = {7},
	urldate = {2020-07-04},
	journal = {Attention, perception \& psychophysics},
	author = {Woods, Kevin J.P. and Siegel, Max and Traer, James and McDermott, Josh H.},
	month = oct,
	year = {2017},
	pmid = {28695541},
	pmcid = {PMC5693749},
	pages = {2064--2072},
	file = {PubMed Central Full Text PDF:C\:\\Users\\bonmc643\\Zotero\\storage\\TZ3DQHGZ\\Woods et al. - 2017 - Headphone screening to facilitate web-based audito.pdf:application/pdf}
}

